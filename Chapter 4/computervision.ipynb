{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the following steps to set up an Azure Computer Vision service. In this chapter, we will use the [Azure CLI a lot to create and delete our resources](https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account-cli?tabs=linux). Cognitive Service you setup, then you might be asked to provision the service using the Azure Portal as you need to accept the Responsible AI terms. You will likely see this error message:\n",
    "```\n",
    "This subscription cannot create ComputerVision until you agree to Responsible AI terms for this resource. You can agree to Responsible AI terms by creating a resource through the Azure Portal then trying again. For more detail go to https://go.microsoft.com/fwlink/?linkid=2164911\n",
    "```\n",
    "First, perform a login according to step 1. As you may have more than one subscriptions in your directory, we set the standard subscription that Azure CLI will use in 2. Finally, there are individual setup and destroy scripts for each service.\n",
    "\n",
    "1. `az login`\n",
    "2. `az account set --subscription \"<YOUR SUBSCRIPTION ID>\"`\n",
    "3. `./computervision-setup.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./computervision-setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Services Computer Vision\n",
    "\n",
    "- [Azure-Samples Quickstart code](https://github.com/Azure-Samples/cognitive-services-quickstart-code)\n",
    "- [Azure Cognitive Services Computer Vision SDK for Python](https://docs.microsoft.com/en-us/python/api/overview/azure/cognitiveservices-vision-computervision-readme?view=azure-python)\n",
    "\n",
    "üü• As Python is the de facto standard language in Machine Learning we will make use of it in most cases. To see other SDK in other languages, please refer to Azure-Samples Quickstart code repository (see above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Text recognition\n",
    "\n",
    "- [OCR Quickstart](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/quickstarts-sdk/client-library?tabs=visual-studio&pivots=programming-language-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "\n",
    "region = os.environ[\"ACCOUNT_REGION\"]\n",
    "key = os.environ[\"ACCOUNT_KEY\"]\n",
    "\n",
    "credentials = CognitiveServicesCredentials(key)\n",
    "client = ComputerVisionClient(\n",
    "    endpoint=\"https://\" + region + \".api.cognitive.microsoft.com/\",\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "# set link if you like to use your own image\n",
    "url = \"https://github.com/Azure-Samples/cognitive-services-python-sdk-samples/raw/master/samples/vision/images/make_things_happen.jpg\"\n",
    "raw = True\n",
    "numberOfCharsInOperationId = 36\n",
    "\n",
    "# SDK call\n",
    "rawHttpResponse = client.read(url, language=\"en\", raw=True)\n",
    "\n",
    "# Get ID from returned headers\n",
    "operation_location = rawHttpResponse.headers[\"Operation-Location\"]\n",
    "id_location = len(operation_location) - numberOfCharsInOperationId\n",
    "operation_id = operation_location[id_location:]\n",
    "\n",
    "# Call the \"GET\" API and wait for it to retrieve the results\n",
    "while True:\n",
    "    result = client.get_read_result(operation_id)\n",
    "    if result.status not in ['notStarted', 'running']:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# Get data\n",
    "if result.status == OperationStatusCodes.succeeded:\n",
    "\n",
    "    for line in result.analyze_result.read_results[0].lines:\n",
    "        print(line.text)\n",
    "        print(line.bounding_box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Analysis\n",
    "\n",
    "In this example we want to analyze an image by tags. We use the `analyze_image` method to perform image analyis and set the visual_features property accordingly to what features we would like to have our image analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "import os\n",
    "region = os.environ['ACCOUNT_REGION']\n",
    "key = os.environ['ACCOUNT_KEY']\n",
    "\n",
    "credentials = CognitiveServicesCredentials(key)\n",
    "client = ComputerVisionClient(\n",
    "    endpoint=\"https://\" + region + \".api.cognitive.microsoft.com/\",\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "# set link if you like to use your own image\n",
    "url = \"https://images.pexels.com/photos/338515/pexels-photo-338515.jpeg\"\n",
    "\n",
    "image_analysis = client.analyze_image(\n",
    "    url, visual_features=[VisualFeatureTypes.tags])\n",
    "\n",
    "for tag in image_analysis.tags:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-specific content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"landmarks\"\n",
    "url = \"https://images.pexels.com/photos/1603650/pexels-photo-1603650.jpeg?auto=compress&cs=tinysrgb&h=750&w=1260\"\n",
    "\n",
    "analysis = client.analyze_image_by_domain(domain, url)\n",
    "\n",
    "for landmark in analysis.result[\"landmarks\"]:\n",
    "    print(landmark[\"name\"])\n",
    "    print(landmark[\"confidence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a thumbnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "width = 50\n",
    "height = 50\n",
    "url = \"https://images.pexels.com/photos/1603650/pexels-photo-1603650.jpeg?auto=compress&cs=tinysrgb&h=750&w=1260\"\n",
    "\n",
    "thumbnail = client.generate_thumbnail(width, height, url)\n",
    "\n",
    "for x in thumbnail:\n",
    "    image = Image.open(io.BytesIO(x))\n",
    "\n",
    "# image.save('thumbnail.jpg')\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Analysis\n",
    "\n",
    "üëâüèº https://github.com/Azure-Samples/cognitive-services-spatial-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./cogsrv-destroy.sh"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
